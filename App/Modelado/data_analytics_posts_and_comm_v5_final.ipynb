{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*******************************************************\n",
    "Title: Python notebook to check the word cloud of downloaded posts and related comments\n",
    "\n",
    "Organization: DANE\n",
    "Author: Andrés D. Pérez\n",
    "Version: 2.0\n",
    "Modification date: 08/10/2021\n",
    "Descripción:\n",
    "    [Sec 1] Libraries\n",
    "    [Sec 2] Functions\n",
    "    [Sec 3] Merge all bases\n",
    "    [Sec 4] Dataset exploration\n",
    "    [Sec 5] Text closeness\n",
    "    \n",
    "    Returns:\n",
    "        Posts dataset for each profile\n",
    "*******************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-console",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "light-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data handling\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import collections, os\n",
    "from collections import Counter\n",
    "import re, unicodedata, spacy\n",
    "\n",
    "#Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Directory listing\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#Text processing and analysis \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('spanish'))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "#Date handling\n",
    "import dateparser\n",
    "from datetime import datetime\n",
    "sns.set()\n",
    "date_fmt = '%b %Y'\n",
    "\n",
    "#Graphing\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-broadcast",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coastal-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_preproc(input_str):\n",
    "    '''\n",
    "    Function to preprocess text strings\n",
    "    \n",
    "    Based on:\n",
    "    https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-normalize-in-a-python-unicode-string\n",
    "    https://stackoverflow.com/questions/5843518/remove-all-special-characters-punctuation-and-spaces-from-string\n",
    "    \n",
    "    Receive a text string in string format called `` input_str``\n",
    "    \n",
    "    First remove special characters\n",
    "    Remove accents and unicode characters\n",
    "    Finally with the method .to_lower () convert the characters to lowercase\n",
    "    \n",
    "    Args:\n",
    "        input_str (string):   Text string to preprocess\n",
    "    Returns:\n",
    "        nfkd_form (string):  Preprocessed text string\n",
    "    '''\n",
    "    \n",
    "    nfkd_form = re.sub(r'[?|$|.|!]',r'',input_str) #Remove special chars\n",
    "    nfkd_form = unicodedata.normalize('NFKD', nfkd_form) #Remove accents\n",
    "    nfkd_form = u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)]) #Remove unicode chars\n",
    "    nfkd_form = (re.sub(r'[^a-zA-Z0-9 ]',r'',nfkd_form)).lower() #Remove missing special chars and convert to lowercase\n",
    "    \n",
    "    return nfkd_form\n",
    "\n",
    "def get_sim_value(in_text, in_query, th=0.5):\n",
    "    \"\"\"\n",
    "    Function to get the indicators\n",
    "    \n",
    "    For description please refer to subsection \n",
    "    \n",
    "    Args:\n",
    "        in_text (string):   Text string to preprocess\n",
    "        in_query (list):   List of string\n",
    "        th (float):   Threshold to filter indicators\n",
    "    Returns:\n",
    "        nfkd_form (string):  Preprocessed text string\n",
    "    \n",
    "    \"\"\"\n",
    "    #Get string and query as list\n",
    "    try:\n",
    "        in_text = str(in_text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    results = []\n",
    "    #--------------------\n",
    "    ucp = in_text.split()\n",
    "    filtered_ucp = [word for word in ucp if word not in stopwords]\n",
    "    \n",
    "    w_vect = []\n",
    "    for w1 in in_query:\n",
    "        for w2 in filtered_ucp:\n",
    "            w_vect.append((SequenceMatcher(None, str(w1), str(w2))).ratio())\n",
    "    \n",
    "    e_vect = []\n",
    "    for el in w_vect:\n",
    "        if el>=th:\n",
    "            e_vect.append(el)\n",
    "            \n",
    "    w_array = np.asarray(w_vect)\n",
    "    e_array = np.asarray(e_vect)\n",
    "    #--Get values--\n",
    "    w_array_sum = w_array.sum()\n",
    "    w_array_mean = w_array.mean()\n",
    "    w_array_median = np.median(w_array)\n",
    "    \n",
    "    if (len(w_array) == 0):\n",
    "        w_array_max = 0\n",
    "        w_array_min = 0\n",
    "    else:\n",
    "        w_array_max = w_array.max()\n",
    "        w_array_min = w_array.min()\n",
    "        \n",
    "    e_array_sum = e_array.sum()\n",
    "    e_array_mean = e_array.mean()\n",
    "    e_array_median = np.median(e_array)\n",
    "    \n",
    "    if (len(e_array) == 0):\n",
    "        e_array_min = 0\n",
    "    else:\n",
    "        e_array_min = e_array.min()\n",
    "        \n",
    "    e_array_tot = len(e_array)\n",
    "    \n",
    "    results = [w_array_sum, w_array_mean, w_array_median, e_array_sum, e_array_mean, e_array_median,\n",
    "               w_array_max, w_array_min, e_array_min, e_array_tot]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-nurse",
   "metadata": {},
   "source": [
    "## 2.1. The ``get_sim_value`` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-sandwich",
   "metadata": {},
   "source": [
    "This function calculates 10 indicators for each discrimination type based on sintactic similarity.\n",
    "\n",
    "This indicators are:\n",
    "- No threshold (sum, mean & median)\n",
    "- Threshold (sum, mean & median)\n",
    "- Max term value\n",
    "- Min term value\n",
    "- Min term value with threshold\n",
    "- Total terms with threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-alberta",
   "metadata": {},
   "source": [
    "The \"star\" of this function is the ``SecuenceMatcher`` class from the ``difflib`` library.\n",
    "\n",
    "This is an algorithm based on the one published in 1980 by Ratcliff and Obershelp under the name \"gestalt pattern matching.\"\n",
    "\n",
    "The idea is to find the longest contiguous matching subsequence that contains no \"garbage\" elements; These \"junk\" items are the ones that are not interesting in some sense, such as blank lines or blank spaces.\n",
    "\n",
    "For a detailed explination visit: https://docs.python.org/3/library/difflib.html or https://towardsdatascience.com/sequencematcher-in-python-6b1e6f3915fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "found-azerbaijan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference word vs Reference word 1.0\n",
      "Reference word vs word 1 0.8571428571428571\n",
      "Reference word vs word 2 0.7692307692307693\n",
      "Reference word vs word 3 0.5454545454545454\n",
      "Reference word vs word 4 0.4\n",
      "Reference word vs word 5 0.0\n"
     ]
    }
   ],
   "source": [
    "#Lets take a look for its use before continue\n",
    "word_ref = 'Machine'\n",
    "word_1 = 'machine'\n",
    "word_2 = 'machin'\n",
    "word_3 = 'mach'\n",
    "word_4 = 'learning'\n",
    "word_5 = 'robot'\n",
    "\n",
    "print('Reference word vs Reference word', SequenceMatcher(None, word_ref, word_ref).ratio())\n",
    "print('Reference word vs word 1', SequenceMatcher(None, word_ref, word_1).ratio())\n",
    "print('Reference word vs word 2', SequenceMatcher(None, word_ref, word_2).ratio())\n",
    "print('Reference word vs word 3', SequenceMatcher(None, word_ref, word_3).ratio())\n",
    "print('Reference word vs word 4', SequenceMatcher(None, word_ref, word_4).ratio())\n",
    "print('Reference word vs word 5', SequenceMatcher(None, word_ref, word_5).ratio())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-aggregate",
   "metadata": {},
   "source": [
    "As you noticed, the more similar the second word is with respect to the reference word, a value closer to 1 will be obtained.\n",
    "\n",
    "Explained this, lets talk about the function using an example. Supose you have the following text: ``My pet fly, has feathers and has a beak``.\n",
    "\n",
    "You want to check if that pet is land or air one, so you have related terms both for **land** and **air** as follows:\n",
    "\n",
    "land: ``fur``, ``fangs``, ``walk`` and ``snout``.\n",
    "\n",
    "air: ``fly``, ``feathers`` and ``beak``.\n",
    "\n",
    "As you noticed we hava a ``in_text (string)`` and two ``in_query (list)``. \n",
    "\n",
    "1. The function takes the text, removes the stop words so the text now works as ``[pet, fly, feathers, beak]``\n",
    "2. Now takes the first query **land** (which have 4 terms) and compares as follows\n",
    "    - ``fur`` vs ``pet``\n",
    "    - ``fur`` vs ``fly``\n",
    "    - ``fur`` vs ``feathers``\n",
    "    - ``fur`` vs ``beak``\n",
    "    - ``fangs`` vs ``pet``\n",
    "    - ``fangs`` vs ``fly``\n",
    "    - ``fangs`` vs ``feathers``\n",
    "    - ``fangs`` vs ``beak``\n",
    "    - ``walk`` vs ``pet``\n",
    "    - ``walk`` vs ``fly``\n",
    "    - ``walk`` vs ``feathers``\n",
    "    - ``walk`` vs ``beak``\n",
    "    - ``snout`` vs ``pet``\n",
    "    - ``snout`` vs ``fly``\n",
    "    - ``snout`` vs ``feathers``\n",
    "    - ``snout`` vs ``beak``\n",
    "3. For each comparison were obtained values (16 values in this case) which are stored in an array (the one termed as ``w_vect``\n",
    "    $$w_{vect} = [val_{1}, val_{2}, val_{3}, ... , val_{n}]$$\n",
    "    - Being $n$ the product of $(number-of-query-terms) \\times (number-of-terms-in-text-after-removing-stop-words)$\n",
    "4. The values in this vector are added obtaining the ``w_array_sum`` value\n",
    "5. The mean over the values of this vector is calculated obtaining ``w_array_mean`` value\n",
    "6. The meadian over the values of this vector is calculated obtaining ``w_array_median`` value\n",
    "7. A second vector is obtained termed ``e_vect``.\n",
    "    - The values in this vector are those values > the threshold value\n",
    "8. As in the ``w_vect`` sum, mean and median are calculated over ``e_vect`` obtaining ``e_array_sum``, ``e_array_mean`` and ``e_array_median`` value respectively.\n",
    "9. Finally, ``w_array_max`` value corresponds to the ``max_value`` in ``w_vect``.\n",
    "10. ``w_array_min`` value corresponds to the ``min_value`` in ``w_vect``.\n",
    "11. ``e_array_min`` value corresponds to the ``min_value`` in ``e_vect``.\n",
    "12. ``e_array_tot`` value corresponds to the ``total of elements`` in ``e_vect``.\n",
    "\n",
    "13. Those values are sorted as a list in ``results`` variable to be added to the list of features.\n",
    "14. The process is repeated for each ``query_list`` (in this case **land** and **air** or in our project case, the 7 discrimination types).\n",
    "15. Then are added to the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-alloy",
   "metadata": {},
   "source": [
    "# 3. Merge all bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "medieval-toronto",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_AlcaldeJorgeMendoza_0.csv',\n",
       " 'base_AlvaroUribeVel_0.csv',\n",
       " 'base_AlvaroUribeVel_1.csv',\n",
       " 'base_AMIGOSDEALEXCAMPOS_0.csv',\n",
       " 'base_andreshurtadoalcalde_0.csv',\n",
       " 'base_andreshurtadoalcalde_1.csv',\n",
       " 'base_andreshurtadoalcalde_2.csv',\n",
       " 'base_andreshurtadoalcalde_3.csv',\n",
       " 'base_andreshurtadoalcalde_4.csv',\n",
       " 'base_andreshurtadoalcalde_5.csv',\n",
       " 'base_Antonio-Caballero-58050573565_0.csv',\n",
       " 'base_BancoDavivienda_0.csv',\n",
       " 'base_Bancolombia_0.csv',\n",
       " 'base_CanalRCN_0.csv',\n",
       " 'base_carlospenagos_si_0.csv',\n",
       " 'base_ClaudiaLopezCL_0.csv',\n",
       " 'base_DANEColombia_0.csv',\n",
       " 'base_departamentonacionaldeplaneacion_0.csv',\n",
       " 'base_DeportivoCaliOficial_0.csv',\n",
       " 'base_DIANCol_0.csv',\n",
       " 'base_DQuinteroCalle_0.csv',\n",
       " 'base_EdgarTovarPedraza_0.csv',\n",
       " 'base_ejercitocolombia_0.csv',\n",
       " 'base_elespectadorcom_0.csv',\n",
       " 'base_eltiempo_0.csv',\n",
       " 'base_FCFSeleccionColPage_0.csv',\n",
       " 'base_FiscaliaCol_0.csv',\n",
       " 'base_gustavopetrourrego_0.csv',\n",
       " 'base_icetexcolombia_0.csv',\n",
       " 'base_independiente_santafe_0.csv',\n",
       " 'base_ivanduquemarquez_0.csv',\n",
       " 'base_JuanManSantosC_0.csv',\n",
       " 'base_juanpisgonzalezWTF_0.csv',\n",
       " 'base_JuniorClubSA_0.csv',\n",
       " 'base_millosfcoficial_0.csv',\n",
       " 'base_MinisterioDeHaciendaYCreditoPublico_0.csv',\n",
       " 'base_NoticiasCaracol_0.csv',\n",
       " 'base_oncecaldasoficial_0.csv',\n",
       " 'base_parquemundoaventura_0.csv',\n",
       " 'base_ParqueSalitreMagico_0.csv',\n",
       " 'base_RegistraduriaNacional_0.csv',\n",
       " 'base_RevistaSemana_0.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List the available files(posts) to concatenate\n",
    "mypath = './bases/popular_base/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brave-router",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "killing-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [00:04<00:00,  9.68it/s]\n"
     ]
    }
   ],
   "source": [
    "#All bases are concatenated\n",
    "all_base_list = []\n",
    "\n",
    "for base in tqdm(onlyfiles):\n",
    "    post_df = pd.read_csv(mypath+base, index_col=0)\n",
    "    all_base_list.append(post_df)\n",
    "    \n",
    "full_df_posts = pd.concat(all_base_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electronic-realtor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_time</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>user_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Luz Marina Cuevas Valderrama</td>\n",
       "      <td>Que triste noticia, lamentablemente su falleci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Ana Josef Aparicio</td>\n",
       "      <td>Descansa en paz mi doctor de los leticianos ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Jorge Enrique Peña</td>\n",
       "      <td>Descanse    en   Paz    gran   PROFESIONAL    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Yolanda Murayari</td>\n",
       "      <td>Qué mi Dios lo tenga en su Santo Reino,paz en ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Maruja Quiñonez</td>\n",
       "      <td>Hay luto y trztesa en mi corazon con la partid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>2020-09-26 19:20:13</td>\n",
       "      <td>1.016039e+16</td>\n",
       "      <td>\"Con la pandemia, con las evidencias de inesta...</td>\n",
       "      <td>Ninna Mezza</td>\n",
       "      <td>No, las universidades se van a quedar sin estu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>2020-09-26 19:20:13</td>\n",
       "      <td>1.016039e+16</td>\n",
       "      <td>\"Con la pandemia, con las evidencias de inesta...</td>\n",
       "      <td>Diego Fernando Escobar Garcia</td>\n",
       "      <td>Peluca o transplante?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520</th>\n",
       "      <td>2020-09-26 19:20:13</td>\n",
       "      <td>1.016039e+16</td>\n",
       "      <td>\"Con la pandemia, con las evidencias de inesta...</td>\n",
       "      <td>Sebastián Mazo</td>\n",
       "      <td>... a maquillarnos?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>2020-09-26 19:20:13</td>\n",
       "      <td>1.016039e+16</td>\n",
       "      <td>\"Con la pandemia, con las evidencias de inesta...</td>\n",
       "      <td>Andres Almeida</td>\n",
       "      <td>Pensé que estaba mostrando una empanada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4522</th>\n",
       "      <td>2020-09-26 19:20:13</td>\n",
       "      <td>1.016039e+16</td>\n",
       "      <td>\"Con la pandemia, con las evidencias de inesta...</td>\n",
       "      <td>Juan David Castro</td>\n",
       "      <td>Jajajaja</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>886849 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                post_time       post_id  \\\n",
       "0     2021-03-22 17:36:51  7.561767e+14   \n",
       "1     2021-03-22 17:36:51  7.561767e+14   \n",
       "2     2021-03-22 17:36:51  7.561767e+14   \n",
       "3     2021-03-22 17:36:51  7.561767e+14   \n",
       "4     2021-03-22 17:36:51  7.561767e+14   \n",
       "...                   ...           ...   \n",
       "4518  2020-09-26 19:20:13  1.016039e+16   \n",
       "4519  2020-09-26 19:20:13  1.016039e+16   \n",
       "4520  2020-09-26 19:20:13  1.016039e+16   \n",
       "4521  2020-09-26 19:20:13  1.016039e+16   \n",
       "4522  2020-09-26 19:20:13  1.016039e+16   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Hoy lamento profundamente el fallecimiento del...   \n",
       "1     Hoy lamento profundamente el fallecimiento del...   \n",
       "2     Hoy lamento profundamente el fallecimiento del...   \n",
       "3     Hoy lamento profundamente el fallecimiento del...   \n",
       "4     Hoy lamento profundamente el fallecimiento del...   \n",
       "...                                                 ...   \n",
       "4518  \"Con la pandemia, con las evidencias de inesta...   \n",
       "4519  \"Con la pandemia, con las evidencias de inesta...   \n",
       "4520  \"Con la pandemia, con las evidencias de inesta...   \n",
       "4521  \"Con la pandemia, con las evidencias de inesta...   \n",
       "4522  \"Con la pandemia, con las evidencias de inesta...   \n",
       "\n",
       "                               user  \\\n",
       "0      Luz Marina Cuevas Valderrama   \n",
       "1                Ana Josef Aparicio   \n",
       "2                Jorge Enrique Peña   \n",
       "3                  Yolanda Murayari   \n",
       "4                   Maruja Quiñonez   \n",
       "...                             ...   \n",
       "4518                    Ninna Mezza   \n",
       "4519  Diego Fernando Escobar Garcia   \n",
       "4520                 Sebastián Mazo   \n",
       "4521                 Andres Almeida   \n",
       "4522              Juan David Castro   \n",
       "\n",
       "                                           user_comment  \n",
       "0     Que triste noticia, lamentablemente su falleci...  \n",
       "1     Descansa en paz mi doctor de los leticianos ve...  \n",
       "2     Descanse    en   Paz    gran   PROFESIONAL    ...  \n",
       "3     Qué mi Dios lo tenga en su Santo Reino,paz en ...  \n",
       "4     Hay luto y trztesa en mi corazon con la partid...  \n",
       "...                                                 ...  \n",
       "4518  No, las universidades se van a quedar sin estu...  \n",
       "4519                              Peluca o transplante?  \n",
       "4520                                ... a maquillarnos?  \n",
       "4521            Pensé que estaba mostrando una empanada  \n",
       "4522                                           Jajajaja  \n",
       "\n",
       "[886849 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "patient-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets save the complete posts dataset\n",
    "full_df_posts.to_csv('./bases/all_posts_comments_fb_raw_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-trustee",
   "metadata": {},
   "source": [
    "# 4. Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "identified-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:  886849 records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_time</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>user_comment</th>\n",
       "      <th>text_proc</th>\n",
       "      <th>user_comment_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Luz Marina Cuevas Valderrama</td>\n",
       "      <td>Que triste noticia, lamentablemente su falleci...</td>\n",
       "      <td>ok</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Ana Josef Aparicio</td>\n",
       "      <td>Descansa en paz mi doctor de los leticianos ve...</td>\n",
       "      <td>ok</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Jorge Enrique Peña</td>\n",
       "      <td>Descanse    en   Paz    gran   PROFESIONAL    ...</td>\n",
       "      <td>ok</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Yolanda Murayari</td>\n",
       "      <td>Qué mi Dios lo tenga en su Santo Reino,paz en ...</td>\n",
       "      <td>ok</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Maruja Quiñonez</td>\n",
       "      <td>Hay luto y trztesa en mi corazon con la partid...</td>\n",
       "      <td>ok</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             post_time       post_id  \\\n",
       "0  2021-03-22 17:36:51  7.561767e+14   \n",
       "1  2021-03-22 17:36:51  7.561767e+14   \n",
       "2  2021-03-22 17:36:51  7.561767e+14   \n",
       "3  2021-03-22 17:36:51  7.561767e+14   \n",
       "4  2021-03-22 17:36:51  7.561767e+14   \n",
       "\n",
       "                                                text  \\\n",
       "0  Hoy lamento profundamente el fallecimiento del...   \n",
       "1  Hoy lamento profundamente el fallecimiento del...   \n",
       "2  Hoy lamento profundamente el fallecimiento del...   \n",
       "3  Hoy lamento profundamente el fallecimiento del...   \n",
       "4  Hoy lamento profundamente el fallecimiento del...   \n",
       "\n",
       "                           user  \\\n",
       "0  Luz Marina Cuevas Valderrama   \n",
       "1            Ana Josef Aparicio   \n",
       "2            Jorge Enrique Peña   \n",
       "3              Yolanda Murayari   \n",
       "4               Maruja Quiñonez   \n",
       "\n",
       "                                        user_comment text_proc  \\\n",
       "0  Que triste noticia, lamentablemente su falleci...        ok   \n",
       "1  Descansa en paz mi doctor de los leticianos ve...        ok   \n",
       "2  Descanse    en   Paz    gran   PROFESIONAL    ...        ok   \n",
       "3  Qué mi Dios lo tenga en su Santo Reino,paz en ...        ok   \n",
       "4  Hay luto y trztesa en mi corazon con la partid...        ok   \n",
       "\n",
       "  user_comment_proc  \n",
       "0                ok  \n",
       "1                ok  \n",
       "2                ok  \n",
       "3                ok  \n",
       "4                ok  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The dataset is loaded\n",
    "fb_base_posts = pd.read_csv('./bases/all_posts_comments_fb_raw_final.csv')\n",
    "fb_base_posts['text_proc'] = 'ok'\n",
    "fb_base_posts['user_comment_proc'] = 'ok'\n",
    "print('Found: ', len(fb_base_posts), 'records')\n",
    "fb_base_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "virtual-frequency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:  52766 NaN registers in text column\n",
      "Filling NaN with no_text\n"
     ]
    }
   ],
   "source": [
    "# Looking for nan in text\n",
    "if (fb_base_posts['text'].isnull().values.any()):\n",
    "    tot_nan = fb_base_posts['text'].isnull().sum()\n",
    "    fb_base_posts['text'].fillna('no_text_in_this_field', inplace=True)\n",
    "    print('Found: ', tot_nan, 'NaN registers in text column')\n",
    "    print('Filling NaN with no_text')\n",
    "else:\n",
    "    print('No NaN records found...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "military-passion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:  66836 NaN registers in user_comment column\n",
      "Filling NaN with no_comment\n"
     ]
    }
   ],
   "source": [
    "# Looking for nan in user_comment\n",
    "if (fb_base_posts['user_comment'].isnull().values.any()):\n",
    "    tot_nan = fb_base_posts['user_comment'].isnull().sum()\n",
    "    fb_base_posts['user_comment'].fillna('no_comment_in_this_field', inplace=True)\n",
    "    print('Found: ', tot_nan, 'NaN registers in user_comment column')\n",
    "    print('Filling NaN with no_comment')\n",
    "else:\n",
    "    print('No NaN records found...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "excited-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = len(fb_base_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cloudy-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_base_posts = fb_base_posts[fb_base_posts['text']!='no_text_in_this_field']\n",
    "fb_base_posts = fb_base_posts[fb_base_posts['user_comment']!='no_comment_in_this_field']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "modern-oakland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115347 regs removed from bse\n"
     ]
    }
   ],
   "source": [
    "fitler_len = len(fb_base_posts)\n",
    "delta_len = orig_len-fitler_len\n",
    "\n",
    "print(delta_len, 'regs removed from bse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "parental-tournament",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 0/771502 [00:00<?, ?it/s]C:\\Users\\user\\anaconda3\\envs\\py38dane\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 771502/771502 [2:28:41<00:00, 86.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing text\n",
    "for row in tqdm(range(len(fb_base_posts['text']))):\n",
    "    df_text = fb_base_posts['text'].iloc[row]\n",
    "    df_text = txt_preproc(df_text)\n",
    "    \n",
    "    df_comment = fb_base_posts['user_comment'].iloc[row]\n",
    "    df_comment = txt_preproc(df_comment)\n",
    "    \n",
    "    fb_base_posts['text_proc'].iloc[row] = df_text\n",
    "    fb_base_posts['user_comment_proc'].iloc[row] = df_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "arbitrary-romania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_time</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>user_comment</th>\n",
       "      <th>text_proc</th>\n",
       "      <th>user_comment_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Luz Marina Cuevas Valderrama</td>\n",
       "      <td>Que triste noticia, lamentablemente su falleci...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>que triste noticia lamentablemente su fallecim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Ana Josef Aparicio</td>\n",
       "      <td>Descansa en paz mi doctor de los leticianos ve...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>descansa en paz mi doctor de los leticianos ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Jorge Enrique Peña</td>\n",
       "      <td>Descanse    en   Paz    gran   PROFESIONAL    ...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>descanse    en   paz    gran   profesional    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Yolanda Murayari</td>\n",
       "      <td>Qué mi Dios lo tenga en su Santo Reino,paz en ...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>que mi dios lo tenga en su santo reinopaz en s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Maruja Quiñonez</td>\n",
       "      <td>Hay luto y trztesa en mi corazon con la partid...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>hay luto y trztesa en mi corazon con la partid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             post_time       post_id  \\\n",
       "0  2021-03-22 17:36:51  7.561767e+14   \n",
       "1  2021-03-22 17:36:51  7.561767e+14   \n",
       "2  2021-03-22 17:36:51  7.561767e+14   \n",
       "3  2021-03-22 17:36:51  7.561767e+14   \n",
       "4  2021-03-22 17:36:51  7.561767e+14   \n",
       "\n",
       "                                                text  \\\n",
       "0  Hoy lamento profundamente el fallecimiento del...   \n",
       "1  Hoy lamento profundamente el fallecimiento del...   \n",
       "2  Hoy lamento profundamente el fallecimiento del...   \n",
       "3  Hoy lamento profundamente el fallecimiento del...   \n",
       "4  Hoy lamento profundamente el fallecimiento del...   \n",
       "\n",
       "                           user  \\\n",
       "0  Luz Marina Cuevas Valderrama   \n",
       "1            Ana Josef Aparicio   \n",
       "2            Jorge Enrique Peña   \n",
       "3              Yolanda Murayari   \n",
       "4               Maruja Quiñonez   \n",
       "\n",
       "                                        user_comment  \\\n",
       "0  Que triste noticia, lamentablemente su falleci...   \n",
       "1  Descansa en paz mi doctor de los leticianos ve...   \n",
       "2  Descanse    en   Paz    gran   PROFESIONAL    ...   \n",
       "3  Qué mi Dios lo tenga en su Santo Reino,paz en ...   \n",
       "4  Hay luto y trztesa en mi corazon con la partid...   \n",
       "\n",
       "                                           text_proc  \\\n",
       "0  hoy lamento profundamente el fallecimiento del...   \n",
       "1  hoy lamento profundamente el fallecimiento del...   \n",
       "2  hoy lamento profundamente el fallecimiento del...   \n",
       "3  hoy lamento profundamente el fallecimiento del...   \n",
       "4  hoy lamento profundamente el fallecimiento del...   \n",
       "\n",
       "                                   user_comment_proc  \n",
       "0  que triste noticia lamentablemente su fallecim...  \n",
       "1  descansa en paz mi doctor de los leticianos ve...  \n",
       "2  descanse    en   paz    gran   profesional    ...  \n",
       "3  que mi dios lo tenga en su santo reinopaz en s...  \n",
       "4  hay luto y trztesa en mi corazon con la partid...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_base_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "periodic-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving preproc FB posts and comments\n",
    "fb_base_posts.to_csv('./bases/all_posts_comments_fb_preproc_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-knight",
   "metadata": {},
   "source": [
    "# 4.1. Wordcloud - Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "metropolitan-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_time</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>user_comment</th>\n",
       "      <th>text_proc</th>\n",
       "      <th>user_comment_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Luz Marina Cuevas Valderrama</td>\n",
       "      <td>Que triste noticia, lamentablemente su falleci...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>que triste noticia lamentablemente su fallecim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Ana Josef Aparicio</td>\n",
       "      <td>Descansa en paz mi doctor de los leticianos ve...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>descansa en paz mi doctor de los leticianos ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Jorge Enrique Peña</td>\n",
       "      <td>Descanse    en   Paz    gran   PROFESIONAL    ...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>descanse    en   paz    gran   profesional    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Yolanda Murayari</td>\n",
       "      <td>Qué mi Dios lo tenga en su Santo Reino,paz en ...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>que mi dios lo tenga en su santo reinopaz en s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Maruja Quiñonez</td>\n",
       "      <td>Hay luto y trztesa en mi corazon con la partid...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>hay luto y trztesa en mi corazon con la partid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771497</th>\n",
       "      <td>2020-09-26 19:20:13</td>\n",
       "      <td>1.016039e+16</td>\n",
       "      <td>\"Con la pandemia, con las evidencias de inesta...</td>\n",
       "      <td>Ninna Mezza</td>\n",
       "      <td>No, las universidades se van a quedar sin estu...</td>\n",
       "      <td>con la pandemia con las evidencias de inestabi...</td>\n",
       "      <td>no las universidades se van a quedar sin estud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771498</th>\n",
       "      <td>2020-09-26 19:20:13</td>\n",
       "      <td>1.016039e+16</td>\n",
       "      <td>\"Con la pandemia, con las evidencias de inesta...</td>\n",
       "      <td>Diego Fernando Escobar Garcia</td>\n",
       "      <td>Peluca o transplante?</td>\n",
       "      <td>con la pandemia con las evidencias de inestabi...</td>\n",
       "      <td>peluca o transplante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771499</th>\n",
       "      <td>2020-09-26 19:20:13</td>\n",
       "      <td>1.016039e+16</td>\n",
       "      <td>\"Con la pandemia, con las evidencias de inesta...</td>\n",
       "      <td>Sebastián Mazo</td>\n",
       "      <td>... a maquillarnos?</td>\n",
       "      <td>con la pandemia con las evidencias de inestabi...</td>\n",
       "      <td>a maquillarnos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771500</th>\n",
       "      <td>2020-09-26 19:20:13</td>\n",
       "      <td>1.016039e+16</td>\n",
       "      <td>\"Con la pandemia, con las evidencias de inesta...</td>\n",
       "      <td>Andres Almeida</td>\n",
       "      <td>Pensé que estaba mostrando una empanada</td>\n",
       "      <td>con la pandemia con las evidencias de inestabi...</td>\n",
       "      <td>pense que estaba mostrando una empanada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771501</th>\n",
       "      <td>2020-09-26 19:20:13</td>\n",
       "      <td>1.016039e+16</td>\n",
       "      <td>\"Con la pandemia, con las evidencias de inesta...</td>\n",
       "      <td>Juan David Castro</td>\n",
       "      <td>Jajajaja</td>\n",
       "      <td>con la pandemia con las evidencias de inestabi...</td>\n",
       "      <td>jajajaja</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>771502 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  post_time       post_id  \\\n",
       "0       2021-03-22 17:36:51  7.561767e+14   \n",
       "1       2021-03-22 17:36:51  7.561767e+14   \n",
       "2       2021-03-22 17:36:51  7.561767e+14   \n",
       "3       2021-03-22 17:36:51  7.561767e+14   \n",
       "4       2021-03-22 17:36:51  7.561767e+14   \n",
       "...                     ...           ...   \n",
       "771497  2020-09-26 19:20:13  1.016039e+16   \n",
       "771498  2020-09-26 19:20:13  1.016039e+16   \n",
       "771499  2020-09-26 19:20:13  1.016039e+16   \n",
       "771500  2020-09-26 19:20:13  1.016039e+16   \n",
       "771501  2020-09-26 19:20:13  1.016039e+16   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Hoy lamento profundamente el fallecimiento del...   \n",
       "1       Hoy lamento profundamente el fallecimiento del...   \n",
       "2       Hoy lamento profundamente el fallecimiento del...   \n",
       "3       Hoy lamento profundamente el fallecimiento del...   \n",
       "4       Hoy lamento profundamente el fallecimiento del...   \n",
       "...                                                   ...   \n",
       "771497  \"Con la pandemia, con las evidencias de inesta...   \n",
       "771498  \"Con la pandemia, con las evidencias de inesta...   \n",
       "771499  \"Con la pandemia, con las evidencias de inesta...   \n",
       "771500  \"Con la pandemia, con las evidencias de inesta...   \n",
       "771501  \"Con la pandemia, con las evidencias de inesta...   \n",
       "\n",
       "                                 user  \\\n",
       "0        Luz Marina Cuevas Valderrama   \n",
       "1                  Ana Josef Aparicio   \n",
       "2                  Jorge Enrique Peña   \n",
       "3                    Yolanda Murayari   \n",
       "4                     Maruja Quiñonez   \n",
       "...                               ...   \n",
       "771497                    Ninna Mezza   \n",
       "771498  Diego Fernando Escobar Garcia   \n",
       "771499                 Sebastián Mazo   \n",
       "771500                 Andres Almeida   \n",
       "771501              Juan David Castro   \n",
       "\n",
       "                                             user_comment  \\\n",
       "0       Que triste noticia, lamentablemente su falleci...   \n",
       "1       Descansa en paz mi doctor de los leticianos ve...   \n",
       "2       Descanse    en   Paz    gran   PROFESIONAL    ...   \n",
       "3       Qué mi Dios lo tenga en su Santo Reino,paz en ...   \n",
       "4       Hay luto y trztesa en mi corazon con la partid...   \n",
       "...                                                   ...   \n",
       "771497  No, las universidades se van a quedar sin estu...   \n",
       "771498                              Peluca o transplante?   \n",
       "771499                                ... a maquillarnos?   \n",
       "771500            Pensé que estaba mostrando una empanada   \n",
       "771501                                           Jajajaja   \n",
       "\n",
       "                                                text_proc  \\\n",
       "0       hoy lamento profundamente el fallecimiento del...   \n",
       "1       hoy lamento profundamente el fallecimiento del...   \n",
       "2       hoy lamento profundamente el fallecimiento del...   \n",
       "3       hoy lamento profundamente el fallecimiento del...   \n",
       "4       hoy lamento profundamente el fallecimiento del...   \n",
       "...                                                   ...   \n",
       "771497  con la pandemia con las evidencias de inestabi...   \n",
       "771498  con la pandemia con las evidencias de inestabi...   \n",
       "771499  con la pandemia con las evidencias de inestabi...   \n",
       "771500  con la pandemia con las evidencias de inestabi...   \n",
       "771501  con la pandemia con las evidencias de inestabi...   \n",
       "\n",
       "                                        user_comment_proc  \n",
       "0       que triste noticia lamentablemente su fallecim...  \n",
       "1       descansa en paz mi doctor de los leticianos ve...  \n",
       "2       descanse    en   paz    gran   profesional    ...  \n",
       "3       que mi dios lo tenga en su santo reinopaz en s...  \n",
       "4       hay luto y trztesa en mi corazon con la partid...  \n",
       "...                                                   ...  \n",
       "771497  no las universidades se van a quedar sin estud...  \n",
       "771498                               peluca o transplante  \n",
       "771499                                     a maquillarnos  \n",
       "771500            pense que estaba mostrando una empanada  \n",
       "771501                                           jajajaja  \n",
       "\n",
       "[771502 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_base_posts = pd.read_csv('./bases/all_posts_comments_fb_preproc_final.csv')\n",
    "fb_base_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fb_base_posts = pd.read_csv('./bases/all_posts_comments_fb_preproc_final.csv')\n",
    "title_words = ''\n",
    "title_list_full, title_list_filtered = [], []\n",
    "\n",
    "# iterate through the csv file \n",
    "for val in fb_base_posts.text_proc: \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "    # split the value \n",
    "    tokens = val.split()\n",
    "    \n",
    "    for tok in tokens:\n",
    "        title_list_full.append(tok)\n",
    "        if tok not in stopwords:\n",
    "            title_list_filtered.append(tok)\n",
    "        \n",
    "    title_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(title_words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (9, 9), facecolor = None) \n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tc_dict = sorted(dict(Counter(title_list_filtered)).items(), key=lambda kv: kv[1])\n",
    "title_array = np.asarray(sorted_tc_dict)\n",
    "\n",
    "labels = np.flipud(title_array[:,0])\n",
    "values = np.flipud(title_array[:,1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Bar(\n",
    "   x = labels[:100],\n",
    "   y = values[:100]\n",
    ")]\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.update_layout(\n",
    "    title_text='Top 100 high-frequency words for texts',\n",
    "    xaxis_title=\"Words\",\n",
    "    yaxis_title=\"Frequency\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-ozone",
   "metadata": {},
   "source": [
    "# 4.2. Wordcloud - Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posts and comments\n",
    "#fb_base_posts = pd.read_csv('./bases/all_posts_comments_fb_preproc_final.csv')\n",
    "title_words = ''\n",
    "title_list_full, title_list_filtered = [], []\n",
    "\n",
    "# iterate through the csv file \n",
    "for val in fb_base_posts.user_comment_proc: \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "    # split the value \n",
    "    tokens = val.split()\n",
    "    \n",
    "    for tok in tokens:\n",
    "        title_list_full.append(tok)\n",
    "        if tok not in stopwords:\n",
    "            title_list_filtered.append(tok)\n",
    "        \n",
    "    title_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(title_words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (9, 9), facecolor = None) \n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tc_dict = sorted(dict(Counter(title_list_filtered)).items(), key=lambda kv: kv[1])\n",
    "title_array = np.asarray(sorted_tc_dict)\n",
    "\n",
    "labels = np.flipud(title_array[:,0])\n",
    "values = np.flipud(title_array[:,1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Bar(\n",
    "   x = labels[:100],\n",
    "   y = values[:100]\n",
    ")]\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.update_layout(\n",
    "    title_text='Top 100 high-frequency words for texts',\n",
    "    xaxis_title=\"Words\",\n",
    "    yaxis_title=\"Frequency\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-headquarters",
   "metadata": {},
   "source": [
    "# 5. Text closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "annoying-forwarding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_time</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>user_comment</th>\n",
       "      <th>text_proc</th>\n",
       "      <th>user_comment_proc</th>\n",
       "      <th>d_eco_sum</th>\n",
       "      <th>d_eco_mean</th>\n",
       "      <th>d_eco_median</th>\n",
       "      <th>...</th>\n",
       "      <th>d_dis_sum</th>\n",
       "      <th>d_dis_mean</th>\n",
       "      <th>d_dis_median</th>\n",
       "      <th>d_dis_sum_th</th>\n",
       "      <th>d_dis_mean_th</th>\n",
       "      <th>d_dis_median_th</th>\n",
       "      <th>d_dis_max</th>\n",
       "      <th>d_dis_min</th>\n",
       "      <th>d_dis_minth</th>\n",
       "      <th>d_dis_tot_th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Luz Marina Cuevas Valderrama</td>\n",
       "      <td>Que triste noticia, lamentablemente su falleci...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>que triste noticia lamentablemente su fallecim...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Ana Josef Aparicio</td>\n",
       "      <td>Descansa en paz mi doctor de los leticianos ve...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>descansa en paz mi doctor de los leticianos ve...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Jorge Enrique Peña</td>\n",
       "      <td>Descanse    en   Paz    gran   PROFESIONAL    ...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>descanse    en   paz    gran   profesional    ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Yolanda Murayari</td>\n",
       "      <td>Qué mi Dios lo tenga en su Santo Reino,paz en ...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>que mi dios lo tenga en su santo reinopaz en s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-22 17:36:51</td>\n",
       "      <td>7.561767e+14</td>\n",
       "      <td>Hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>Maruja Quiñonez</td>\n",
       "      <td>Hay luto y trztesa en mi corazon con la partid...</td>\n",
       "      <td>hoy lamento profundamente el fallecimiento del...</td>\n",
       "      <td>hay luto y trztesa en mi corazon con la partid...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             post_time       post_id  \\\n",
       "0  2021-03-22 17:36:51  7.561767e+14   \n",
       "1  2021-03-22 17:36:51  7.561767e+14   \n",
       "2  2021-03-22 17:36:51  7.561767e+14   \n",
       "3  2021-03-22 17:36:51  7.561767e+14   \n",
       "4  2021-03-22 17:36:51  7.561767e+14   \n",
       "\n",
       "                                                text  \\\n",
       "0  Hoy lamento profundamente el fallecimiento del...   \n",
       "1  Hoy lamento profundamente el fallecimiento del...   \n",
       "2  Hoy lamento profundamente el fallecimiento del...   \n",
       "3  Hoy lamento profundamente el fallecimiento del...   \n",
       "4  Hoy lamento profundamente el fallecimiento del...   \n",
       "\n",
       "                           user  \\\n",
       "0  Luz Marina Cuevas Valderrama   \n",
       "1            Ana Josef Aparicio   \n",
       "2            Jorge Enrique Peña   \n",
       "3              Yolanda Murayari   \n",
       "4               Maruja Quiñonez   \n",
       "\n",
       "                                        user_comment  \\\n",
       "0  Que triste noticia, lamentablemente su falleci...   \n",
       "1  Descansa en paz mi doctor de los leticianos ve...   \n",
       "2  Descanse    en   Paz    gran   PROFESIONAL    ...   \n",
       "3  Qué mi Dios lo tenga en su Santo Reino,paz en ...   \n",
       "4  Hay luto y trztesa en mi corazon con la partid...   \n",
       "\n",
       "                                           text_proc  \\\n",
       "0  hoy lamento profundamente el fallecimiento del...   \n",
       "1  hoy lamento profundamente el fallecimiento del...   \n",
       "2  hoy lamento profundamente el fallecimiento del...   \n",
       "3  hoy lamento profundamente el fallecimiento del...   \n",
       "4  hoy lamento profundamente el fallecimiento del...   \n",
       "\n",
       "                                   user_comment_proc  d_eco_sum  d_eco_mean  \\\n",
       "0  que triste noticia lamentablemente su fallecim...          0           0   \n",
       "1  descansa en paz mi doctor de los leticianos ve...          0           0   \n",
       "2  descanse    en   paz    gran   profesional    ...          0           0   \n",
       "3  que mi dios lo tenga en su santo reinopaz en s...          0           0   \n",
       "4  hay luto y trztesa en mi corazon con la partid...          0           0   \n",
       "\n",
       "   d_eco_median  ...  d_dis_sum  d_dis_mean  d_dis_median  d_dis_sum_th  \\\n",
       "0             0  ...          0           0             0             0   \n",
       "1             0  ...          0           0             0             0   \n",
       "2             0  ...          0           0             0             0   \n",
       "3             0  ...          0           0             0             0   \n",
       "4             0  ...          0           0             0             0   \n",
       "\n",
       "   d_dis_mean_th  d_dis_median_th  d_dis_max  d_dis_min  d_dis_minth  \\\n",
       "0              0                0          0          0            0   \n",
       "1              0                0          0          0            0   \n",
       "2              0                0          0          0            0   \n",
       "3              0                0          0          0            0   \n",
       "4              0                0          0          0            0   \n",
       "\n",
       "   d_dis_tot_th  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The dataset is loaded\n",
    "fb_base_posts = pd.read_csv('./bases/all_posts_comments_fb_preproc_final.csv')\n",
    "\n",
    "#Indicator variables are defined\n",
    "fb_base_new_cols = ['d_eco_sum', 'd_eco_mean', 'd_eco_median', 'd_eco_sum_th', 'd_eco_mean_th', 'd_eco_median_th',\n",
    "                    'd_eco_max', 'd_eco_min', 'd_eco_minth', 'd_eco_tot_th',\n",
    "                    'd_osex_sum', 'd_osex_mean', 'd_osex_median', 'd_osex_sum_th', 'd_osex_mean_th', 'd_osex_median_th',\n",
    "                    'd_osex_max', 'd_osex_min', 'd_osex_minth', 'd_osex_tot_th',\n",
    "                    'd_pol_sum', 'd_pol_mean', 'd_pol_median', 'd_pol_sum_th', 'd_pol_mean_th', 'd_pol_median_th',\n",
    "                    'd_pol_max', 'd_pol_min', 'd_pol_minth', 'd_pol_tot_th',\n",
    "                    'd_mig_sum', 'd_mig_mean', 'd_mig_median', 'd_mig_sum_th', 'd_mig_mean_th', 'd_mig_median_th',\n",
    "                    'd_mig_max', 'd_mig_min', 'd_mig_minth', 'd_mig_tot_th',\n",
    "                    'd_sex_sum', 'd_sex_mean', 'd_sex_median', 'd_sex_sum_th', 'd_sex_mean_th', 'd_sex_median_th',\n",
    "                    'd_sex_max', 'd_sex_min', 'd_sex_minth', 'd_sex_tot_th',\n",
    "                    'd_rac_sum', 'd_rac_mean', 'd_rac_median', 'd_rac_sum_th', 'd_rac_mean_th', 'd_rac_median_th',\n",
    "                    'd_rac_max', 'd_rac_min', 'd_rac_minth', 'd_rac_tot_th',\n",
    "                    'd_dis_sum', 'd_dis_mean', 'd_dis_median', 'd_dis_sum_th', 'd_dis_mean_th', 'd_dis_median_th',\n",
    "                    'd_dis_max', 'd_dis_min', 'd_dis_minth', 'd_dis_tot_th']\n",
    "\n",
    "#Indicator variables are set as 0\n",
    "for n_col in fb_base_new_cols:\n",
    "    fb_base_posts[n_col] = 0\n",
    "    \n",
    "fb_base_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "based-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Previous query terms\n",
    "d_economic = [\"indigente\", \"mendigo\", \"mendiga\", \"vagabundo\", \"vagabunda\", \"arrastrado\", \"arrastrada\",\n",
    "              \"nero\", \"nera\", \"guiso\", \"guisa\", \"pobre\", \"muerto de hambre\", \"pordiosero\", \"pordiosera\",\n",
    "              \"desechable\"]\n",
    "\n",
    "d_racial = [\"negro\", \"negra\", \"indio\", \"india\", \"negrito\", \"indiecito\", \"niche\", \"africa\", \"choco\",\n",
    "            \"chocoano\", \"indigena\", \"afro\", \"afrocolombiano\", \"salvaje\"]\n",
    "\n",
    "d_discap = [\"discapacitado\", \"discapacitada\", \"minusvalido\", \"minusvalida\", \"invalido\", \"invalido\",\n",
    "            \"excepcional\", \"imbecil\", \"idiota\", \"anormal\", \"impedido\", \"impedida\", \"retrasado\", \"cojo\", \"coja\",\n",
    "            \"autista\", \"cieguito\", \"manguito\", \"loquita\"]\n",
    "\n",
    "d_sexo = [\"zorra\", \"perra\", \"verdulera\", \"fulana\", \"bruja\", \"mujerzuela\", \"loba\", \"prostituta\", \"puta\"]\n",
    "\n",
    "d_o_sexual = [\"marica\", \"gay\", \"lgbti\", \"hermafrodita\", \"maricon\", \"lesbiana\",\n",
    "              \"travesti\", \"loquita\", \"rarito\", \"desviado\", \"homosexualismo\", \"lesbianismo\"]\n",
    "\n",
    "d_politic = [\"vandalo\", \"vandala\", \"comunista\", \"mamerto\", \"mamerta\", \"facho\", \"facha\",\n",
    "             \"fascista\", \"tibio\", \"tibia\", \"gente de bien\", \"petrista\", \"uribista\",\n",
    "             \"guerrillero\", \"guerrillera\", \"paraco\", \"paraca\", \"reinsertados\", \"desmovilizados\"]\n",
    "\"\"\"\n",
    "#=============================================================================================\n",
    "#New query terms\n",
    "d_economic = ['indigent', 'mendig', 'vagabund', 'arrastrad', 'ner', 'guis',\n",
    "              'pobre', 'muert de hambre', 'pordioser', 'desechable',\n",
    "              'miserable', 'desvalid', 'mugrient', 'pelafustan', 'limosnero', \n",
    "              'necesitad', 'vag', 'recostad']\n",
    "\n",
    "d_o_sexual = ['marica', 'gay', 'lgbti', 'hermafrodita', 'maricon', 'lesbiana', 'travesti',\n",
    "              'loquita', 'rarit', 'desviad', 'homosexual', 'lesbianismo', 'rosco', 'roscon',\n",
    "              'afeminado', 'marimach', 'machorra', 'arepera', 'pirob', 'machista', \n",
    "              'feminista', 'monosexual', 'pansexualista', 'pasiv', 'activ', 'transexual',\n",
    "              'bollera', 'salir del closet', 'coming out', 'cross-dressing', 'drag',\n",
    "              'hetero', 'queer', 'torcid', 'rosca floja']\n",
    "\n",
    "d_politic = ['vandal', 'comunista', 'mamert', 'fach', 'fascista', 'tibi',\n",
    "             'gente de bien', 'petrista', 'uribista', 'guerriller', 'subversiv',\n",
    "             'parac', 'reinsertados', 'desmovilizados', 'sindicalista', 'socialista',\n",
    "             'derechista', 'izquierdista', 'god', 'furibista', 'zurd',\n",
    "             'dictadura', 'agitador', 'primera linea', 'revolucionario', 'delfin',\n",
    "             'lagart', 'lentej', 'mermelada', 'palanca', 'serrucho', 'voltearepas']\n",
    "\n",
    "d_migra = ['venec', 'extranjer', 'desplazad', 'cham', 'refugiad', 'expatriad', 'apatrida', 'atrasad']\n",
    "\n",
    "d_sexo = ['zorra', 'perra', 'verdulera', 'fulana', 'bruja', 'mujerzuela', 'loba',\n",
    "          'prostituta', 'puta', 'mujercita', 'jovencita', 'ninita', 'señorita',\n",
    "          'feminazi', 'prostituta', 'perdida', 'vida facil', 'nina', 'nena', \n",
    "          'mantenida', 'zunga', 'morronga', 'golfa', 'aventurera', 'callejera',\n",
    "          'atrevida', 'mujer publica', 'vagabunda']\n",
    "\n",
    "d_racial = ['negr', 'indi', 'negrit', 'indiecit', 'niche', 'africa', 'choco',\n",
    "            'chocoano', 'indigena', 'afro', 'afrocolombiano', 'salvaje', 'gitan',\n",
    "            'costen', 'cachac', 'paisa', 'pastus', 'amarill', 'oriental', \n",
    "            'achinad', 'mon', 'gring', 'europe', 'chol', 'cuajada', 'african']\n",
    "\n",
    "d_discap = ['discapacitad', 'minusvalid', 'invalid', 'excepcional', 'imbecil', 'idiota', 'anormal', \n",
    "            'impedid', 'retrasad', 'coj', 'autista', 'cieguito', 'manguito', 'deficiente', 'enferm',\n",
    "            'incapacitad', 'disminuid', 'inutil', 'especial', 'sordomud', 'mud', 'invidente', 'cieg',\n",
    "            'cegaton', 'vista', 'retardado', 'mental', 'mongolic', 'tont', 'bobit', 'incapaz',\n",
    "            'tarad', 'enferm', 'locos', 'demente', 'trastornad', 'depresiv', 'esquizofrenic', 'bipolar',\n",
    "            'loquit', 'paralitic', 'lisiad', 'tullid', 'minusvalid', 'mutilad', 'moch', 'chuequit',\n",
    "            'enan', 'gigante', 'gord', 'obes', 'rellen', 'regordeta']\n",
    "\n",
    "all_dis_list = [d_economic, d_o_sexual, d_politic, d_migra, d_sexo, d_racial, d_discap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incomplete-campus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 0/771502 [00:00<?, ?it/s]C:\\Users\\user\\anaconda3\\envs\\py38dane\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "  0%|                                                                                                      | 7/771502 [00:00<25:53:49,  8.28it/s]<ipython-input-2-389663ee5d1a>:78: RuntimeWarning: Mean of empty slice.\n",
      "  e_array_mean = e_array.mean()\n",
      "C:\\Users\\user\\anaconda3\\envs\\py38dane\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\user\\anaconda3\\envs\\py38dane\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "  0%|▎                                                                                                  | 2550/771502 [04:04<14:24:23, 14.83it/s]<ipython-input-2-389663ee5d1a>:67: RuntimeWarning: Mean of empty slice.\n",
      "  w_array_mean = w_array.mean()\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 771502/771502 [19:29:18<00:00, 11.00it/s]\n"
     ]
    }
   ],
   "source": [
    "#Indicators calculus\n",
    "for row in tqdm(range(len(fb_base_posts))):\n",
    "    user_comment_proc = fb_base_posts['user_comment_proc'].iloc[row]\n",
    "\n",
    "    all_indic_list = []\n",
    "    for disc_type in all_dis_list:\n",
    "        values_disc = get_sim_value(user_comment_proc, disc_type, 0.5)\n",
    "\n",
    "        for indic_val in values_disc:\n",
    "            all_indic_list.append(indic_val)\n",
    "            \n",
    "    for col_name_index in range(len(fb_base_new_cols)):\n",
    "        fb_base_posts[fb_base_new_cols[col_name_index]].iloc[row]=all_indic_list[col_name_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "naked-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving preproc FB posts and comments\n",
    "fb_base_posts.to_csv('./bases/all_posts_comments_fb_indices_final_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-sleeve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "furnished-being",
   "metadata": {},
   "source": [
    "# Closeness review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intimate-blogger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_time</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>user_comment</th>\n",
       "      <th>text_proc</th>\n",
       "      <th>user_comment_proc</th>\n",
       "      <th>d_eco_sum</th>\n",
       "      <th>d_eco_mean</th>\n",
       "      <th>d_eco_median</th>\n",
       "      <th>...</th>\n",
       "      <th>d_rac_median</th>\n",
       "      <th>d_rac_sum_th</th>\n",
       "      <th>d_rac_mean_th</th>\n",
       "      <th>d_rac_median_th</th>\n",
       "      <th>d_dis_sum</th>\n",
       "      <th>d_dis_mean</th>\n",
       "      <th>d_dis_median</th>\n",
       "      <th>d_dis_sum_th</th>\n",
       "      <th>d_dis_mean_th</th>\n",
       "      <th>d_dis_median_th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-03 07:52:06</td>\n",
       "      <td>1.015893e+16</td>\n",
       "      <td>Más aprendices mejor remuneración para ayudar ...</td>\n",
       "      <td>Freddy Vera Perez</td>\n",
       "      <td>Señor presidente, en este momento tan difícil ...</td>\n",
       "      <td>mas aprendices mejor remuneracion para ayudar ...</td>\n",
       "      <td>senor presidente en este momento tan dificil m...</td>\n",
       "      <td>136.378352</td>\n",
       "      <td>0.236768</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>17.930609</td>\n",
       "      <td>0.578407</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>394.802505</td>\n",
       "      <td>0.232784</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>32.699834</td>\n",
       "      <td>0.544997</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-03 07:52:06</td>\n",
       "      <td>1.015893e+16</td>\n",
       "      <td>Más aprendices mejor remuneración para ayudar ...</td>\n",
       "      <td>Francy Elena Ortiz Olaya</td>\n",
       "      <td>Le debe preocupar y mucho, ver cómo sumergió a...</td>\n",
       "      <td>mas aprendices mejor remuneracion para ayudar ...</td>\n",
       "      <td>le debe preocupar y mucho ver como sumergio a ...</td>\n",
       "      <td>148.749832</td>\n",
       "      <td>0.250421</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>13.161381</td>\n",
       "      <td>0.526455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>406.215308</td>\n",
       "      <td>0.232256</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>27.696568</td>\n",
       "      <td>0.532626</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-03 07:52:06</td>\n",
       "      <td>1.015893e+16</td>\n",
       "      <td>Más aprendices mejor remuneración para ayudar ...</td>\n",
       "      <td>Gloria Zapata</td>\n",
       "      <td>Muchos colombianos lo sabemos y siempre será r...</td>\n",
       "      <td>mas aprendices mejor remuneracion para ayudar ...</td>\n",
       "      <td>muchos colombianos lo sabemos y siempre sera r...</td>\n",
       "      <td>75.469591</td>\n",
       "      <td>0.246633</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>11.210876</td>\n",
       "      <td>0.560544</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>202.245521</td>\n",
       "      <td>0.224468</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>16.777844</td>\n",
       "      <td>0.541221</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-03 07:52:06</td>\n",
       "      <td>1.015893e+16</td>\n",
       "      <td>Más aprendices mejor remuneración para ayudar ...</td>\n",
       "      <td>Sirley Beatriz Noriega Herrera</td>\n",
       "      <td>Por lo menos Ud hace,  esté en la situación q ...</td>\n",
       "      <td>mas aprendices mejor remuneracion para ayudar ...</td>\n",
       "      <td>por lo menos ud hace  este en la situacion q e...</td>\n",
       "      <td>40.419357</td>\n",
       "      <td>0.187127</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>6.365079</td>\n",
       "      <td>0.578644</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>113.869950</td>\n",
       "      <td>0.179041</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>5.424242</td>\n",
       "      <td>0.542424</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-03 07:52:06</td>\n",
       "      <td>1.015893e+16</td>\n",
       "      <td>Más aprendices mejor remuneración para ayudar ...</td>\n",
       "      <td>Daniel Valenzuela</td>\n",
       "      <td>Que irónico antes los aprendices ganaban el mí...</td>\n",
       "      <td>mas aprendices mejor remuneracion para ayudar ...</td>\n",
       "      <td>que ironico antes los aprendices ganaban el mi...</td>\n",
       "      <td>111.715179</td>\n",
       "      <td>0.248256</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>16.386922</td>\n",
       "      <td>0.528610</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>304.338962</td>\n",
       "      <td>0.229690</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>25.978436</td>\n",
       "      <td>0.530172</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             post_time       post_id  \\\n",
       "0  2020-10-03 07:52:06  1.015893e+16   \n",
       "1  2020-10-03 07:52:06  1.015893e+16   \n",
       "2  2020-10-03 07:52:06  1.015893e+16   \n",
       "3  2020-10-03 07:52:06  1.015893e+16   \n",
       "4  2020-10-03 07:52:06  1.015893e+16   \n",
       "\n",
       "                                                text  \\\n",
       "0  Más aprendices mejor remuneración para ayudar ...   \n",
       "1  Más aprendices mejor remuneración para ayudar ...   \n",
       "2  Más aprendices mejor remuneración para ayudar ...   \n",
       "3  Más aprendices mejor remuneración para ayudar ...   \n",
       "4  Más aprendices mejor remuneración para ayudar ...   \n",
       "\n",
       "                             user  \\\n",
       "0               Freddy Vera Perez   \n",
       "1        Francy Elena Ortiz Olaya   \n",
       "2                   Gloria Zapata   \n",
       "3  Sirley Beatriz Noriega Herrera   \n",
       "4               Daniel Valenzuela   \n",
       "\n",
       "                                        user_comment  \\\n",
       "0  Señor presidente, en este momento tan difícil ...   \n",
       "1  Le debe preocupar y mucho, ver cómo sumergió a...   \n",
       "2  Muchos colombianos lo sabemos y siempre será r...   \n",
       "3  Por lo menos Ud hace,  esté en la situación q ...   \n",
       "4  Que irónico antes los aprendices ganaban el mí...   \n",
       "\n",
       "                                           text_proc  \\\n",
       "0  mas aprendices mejor remuneracion para ayudar ...   \n",
       "1  mas aprendices mejor remuneracion para ayudar ...   \n",
       "2  mas aprendices mejor remuneracion para ayudar ...   \n",
       "3  mas aprendices mejor remuneracion para ayudar ...   \n",
       "4  mas aprendices mejor remuneracion para ayudar ...   \n",
       "\n",
       "                                   user_comment_proc   d_eco_sum  d_eco_mean  \\\n",
       "0  senor presidente en este momento tan dificil m...  136.378352    0.236768   \n",
       "1  le debe preocupar y mucho ver como sumergio a ...  148.749832    0.250421   \n",
       "2  muchos colombianos lo sabemos y siempre sera r...   75.469591    0.246633   \n",
       "3  por lo menos ud hace  este en la situacion q e...   40.419357    0.187127   \n",
       "4  que ironico antes los aprendices ganaban el mi...  111.715179    0.248256   \n",
       "\n",
       "   d_eco_median  ...  d_rac_median  d_rac_sum_th  d_rac_mean_th  \\\n",
       "0      0.235294  ...      0.222222     17.930609       0.578407   \n",
       "1      0.250000  ...      0.200000     13.161381       0.526455   \n",
       "2      0.250000  ...      0.181818     11.210876       0.560544   \n",
       "3      0.181818  ...      0.181818      6.365079       0.578644   \n",
       "4      0.235294  ...      0.235294     16.386922       0.528610   \n",
       "\n",
       "   d_rac_median_th   d_dis_sum  d_dis_mean  d_dis_median  d_dis_sum_th  \\\n",
       "0         0.545455  394.802505    0.232784      0.222222     32.699834   \n",
       "1         0.500000  406.215308    0.232256      0.222222     27.696568   \n",
       "2         0.500000  202.245521    0.224468      0.222222     16.777844   \n",
       "3         0.571429  113.869950    0.179041      0.181818      5.424242   \n",
       "4         0.500000  304.338962    0.229690      0.222222     25.978436   \n",
       "\n",
       "   d_dis_mean_th  d_dis_median_th  \n",
       "0       0.544997         0.533333  \n",
       "1       0.532626         0.500000  \n",
       "2       0.541221         0.526316  \n",
       "3       0.542424         0.500000  \n",
       "4       0.530172         0.500000  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_base_posts = pd.read_csv('./bases/all_posts_comments_fb_indices.csv')\n",
    "    \n",
    "fb_base_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of Indicators plotting\n",
    "gen_width = 980\n",
    "gen_height = 200\n",
    "\n",
    "x_axis_values = list(fb_base_posts.index)\n",
    "\n",
    "d_eco_sum_vals_th = list(fb_base_posts['d_eco_sum_th'])\n",
    "d_osex_sum_vals_th = list(fb_base_posts['d_osex_sum_th'])\n",
    "d_pol_sum_vals_th = list(fb_base_posts['d_pol_sum_th'])\n",
    "d_mig_sum_vals_th = list(fb_base_posts['d_mig_sum_th'])\n",
    "d_sex_sum_vals_th  = list(fb_base_posts['d_sex_sum_th'])\n",
    "d_rac_sum_vals_th = list(fb_base_posts['d_rac_sum_th'])\n",
    "d_dis_sum_vals_th = list(fb_base_posts['d_dis_sum_th'])\n",
    "\n",
    "d_eco_mean_vals_th = list(fb_base_posts['d_eco_mean_th'])\n",
    "d_osex_mean_vals_th = list(fb_base_posts['d_osex_mean_th'])\n",
    "d_pol_mean_vals_th = list(fb_base_posts['d_pol_mean_th'])\n",
    "d_mig_mean_vals_th = list(fb_base_posts['d_mig_mean_th'])\n",
    "d_sex_mean_vals_th  = list(fb_base_posts['d_sex_mean_th'])\n",
    "d_rac_mean_vals_th = list(fb_base_posts['d_rac_mean_th'])\n",
    "d_dis_mean_vals_th = list(fb_base_posts['d_dis_mean_th'])\n",
    "\n",
    "d_eco_median_vals_th = list(fb_base_posts['d_eco_median_th'])\n",
    "d_osex_median_vals_th = list(fb_base_posts['d_osex_median_th'])\n",
    "d_pol_median_vals_th = list(fb_base_posts['d_pol_median_th'])\n",
    "d_mig_median_vals_th = list(fb_base_posts['d_mig_median_th'])\n",
    "d_sex_median_vals_th  = list(fb_base_posts['d_sex_median_th'])\n",
    "d_rac_median_vals_th = list(fb_base_posts['d_rac_median_th'])\n",
    "d_dis_median_vals_th = list(fb_base_posts['d_dis_median_th'])\n",
    "#===========================================\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_eco_sum_vals_th,\n",
    "    mode='lines', name='Economic', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_osex_sum_vals_th,\n",
    "    mode='lines', name='Sex. Orient', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_pol_sum_vals_th,\n",
    "    mode='lines', name='Politics', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_mig_sum_vals_th,\n",
    "    mode='lines', name='Migration', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_sex_sum_vals_th,\n",
    "    mode='lines', name='Sex', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_rac_sum_vals_th,\n",
    "    mode='lines', name='Racial', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_dis_sum_vals_th,\n",
    "    mode='lines', name='Disability', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=gen_width,\n",
    "    height=gen_height,\n",
    "    margin=dict(l=0, r=0, t=50, b=0),\n",
    "    font=dict(size=10),\n",
    "    title_text=\"Sum indicator for 7 disc with threshold\",\n",
    "    xaxis_title=\"Comment\",\n",
    "    yaxis_title=\"Value\",\n",
    "    legend_title=\"Discrimination\",\n",
    ")\n",
    "fig.show()\n",
    "#-------------------------------------------\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_eco_mean_vals_th,\n",
    "    mode='lines', name='Economic', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_osex_mean_vals_th,\n",
    "    mode='lines', name='Sex. Orient', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_pol_mean_vals_th,\n",
    "    mode='lines', name='Politics', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_mig_mean_vals_th,\n",
    "    mode='lines', name='Migration', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_sex_mean_vals_th,\n",
    "    mode='lines', name='Sex', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_rac_mean_vals_th,\n",
    "    mode='lines', name='Racial', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_dis_mean_vals_th,\n",
    "    mode='lines', name='Disability', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=gen_width,\n",
    "    height=gen_height,\n",
    "    margin=dict(l=0, r=0, t=50, b=0),\n",
    "    font=dict(size=10),\n",
    "    title_text=\"Mean indicator for 7 disc with threshold\",\n",
    "    xaxis_title=\"Comment\",\n",
    "    yaxis_title=\"Value\",\n",
    "    legend_title=\"Discrimination\",\n",
    ")\n",
    "fig.show()\n",
    "#------------------------------------------\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_eco_median_vals_th,\n",
    "    mode='lines', name='Economic', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_osex_median_vals_th,\n",
    "    mode='lines', name='Sex. Orient', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_pol_median_vals_th,\n",
    "    mode='lines', name='Politics', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_mig_median_vals_th,\n",
    "    mode='lines', name='Migration', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_sex_median_vals_th,\n",
    "    mode='lines', name='Sex', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_rac_median_vals_th,\n",
    "    mode='lines', name='Racial', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "fig.append_trace(go.Scatter(\n",
    "    x=x_axis_values, y=d_dis_median_vals_th,\n",
    "    mode='lines', name='Disability', line = dict(width = 1)),\n",
    "    row=1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=gen_width,\n",
    "    height=gen_height,\n",
    "    margin=dict(l=0, r=0, t=50, b=0),\n",
    "    font=dict(size=10),\n",
    "    title_text=\"Median indicator for 7 disc with threshold\",\n",
    "    xaxis_title=\"Comment\",\n",
    "    yaxis_title=\"Value\",\n",
    "    legend_title=\"Discrimination\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-annex",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
